[
  {
    "url": "https://luzbetak.github.io/ai/Python-Syntax-Highlighting.html",
    "title": "Python-Syntax-Highlighting.html",
    "content": "kevin luzbetak computer science python syntax highlighting def greet name print f hello name def add a b return a b if __name__ __ main__",
    "created_at": "2024-11-12T22:25:48.803799",
    "updated_at": "2024-11-12T22:25:48.803802"
  },
  {
    "url": "https://luzbetak.github.io/ai/Keras.html",
    "title": "Keras.html",
    "content": "keras training neural networks keras overview keras is an open source deep learning library that provides a high level api for building and training neural networks it is user friendly modular and extensible allowing developers to create complex models with minimal code keras runs on top of low level deep learning frameworks such as tensorflow theano and cntk making it both versatile and powerful key features ease of use keras offers a simple and consistent api to build neural networks quickly modular it allows you to build models by combining different building blocks layers optimizers loss functions flexibility keras can be used for a variety of tasks including image classification text generation reinforcement learning and more support for both cpu and gpu keras can run on both cpu and gpu hardware enabling faster training when using gpus backed by tensorflow keras is now part of the tensorflow core library making it the default high level api for tensorflow 1 feedforward neural network classification task a feedforward neural network fnn is a type of artificial neural network where the connections between nodes do not form a cycle it is called feedforward because the data flows only in one direction from the input layer to the output layer without any loops or feedback connections structure input layer this layer receives the input features each node in the input layer corresponds to one feature of the data hidden layers these are the layers where most of the computation occurs the hidden layers use activation functions like relu rectified linear unit to introduce non linearity allowing the network to learn complex patterns in the data output layer this layer produces the final prediction for classification tasks the output layer typically uses a sigmoid binary classification or softmax multi class classification activation function to output probabilities for each class example of a classification task in the example of classifying data with synthetic features the network learns to map the input features e g 20 features to the correct class labels 0 or 1 by adjusting the weights and biases during training the process involves forward propagation calculating output and backpropagation adjusting weights based on the loss function and optimizer like adam it is suitable for basic tasks like classifying tabular data use case binary or multi class classification on structured data such as customer churn prediction medical diagnosis or fraud detection import numpy as np from tensorflow keras models import sequential from tensorflow keras layers import dense from sklearn datasets import make_classification from sklearn model_selection import train_test_split from sklearn preprocessing import standardscaler generate synthetic data for classification x y make_classification n_samples 1000 n_features 20 n_classes 2 random_state 42 split the data into training and testing sets x_train x_test y_train y_test train_test_split x y test_size 0 2 random_state 42 normalize the features scaler standardscaler x_train scaler fit_transform x_train x_test scaler transform x_test define the model model sequential model add dense 32 input_dim 20 activation relu input layer model add dense 16 activation relu hidden layer model add dense 1 activation sigmoid output layer for binary classification compile the model model compile optimizer adam loss binary_crossentropy metrics accuracy train the model model fit x_train y_train epochs 10 batch_size 32 validation_data x_test y_test evaluate the model loss accuracy model evaluate x_test y_test print f accuracy accuracy 2f 2 convolutional neural network cnn for image classification a convolutional neural network cnn is a specialized neural network primarily used for processing structured grid like data such as images cnns are particularly effective at recognizing spatial hierarchies in data making them ideal for tasks like image and video recognition structure input layer the input to a cnn is typically a multi dimensional image e g a grayscale image with a size of 28x28 pixels has one channel while a colored image has 3 channels rgb convolutional layers these layers apply convolution operations to the input using filters kernels to detect features like edges corners and textures the output of each convolutional layer is a feature map that highlights these patterns pooling layers these layers reduce the spatial dimensions of the feature maps e g max pooling helping to retain important information while reducing computational complexity flattening after the convolutional and pooling layers the 2d feature maps are flattened into a 1d vector that can be fed into a fully connected layer fully connected layers these layers are similar to a feedforward network and are used to make the final classification decision the output layer typically uses a softmax activation function for multi class classification example of image classification in the mnist digit classification example the network takes a 28x28 pixel grayscale image as input passes it through convolutional layers to detect edges and patterns and eventually predicts the digit class 0 9 using fully connected layers cnns are excellent at reducing the number of parameters by reusing the filters across the image making them highly efficient for image processing tasks use case image classification tasks such as handwriting recognition object detection medical image analysis and facial recognition from tensorflow keras models import sequential from tensorflow keras layers import conv2d maxpooling2d flatten dense from tensorflow keras datasets import mnist from tensorflow keras utils import to_categorical load the mnist dataset handwritten digits x_train y_train",
    "created_at": "2024-11-12T22:25:49.921987",
    "updated_at": "2024-11-12T22:25:49.921990"
  },
  {
    "url": "https://luzbetak.github.io/ai/python-whoosh.html",
    "title": "python-whoosh.html",
    "content": "python whoosh create python bm25 index usr bin env python from whoosh index import create_in from whoosh fields import schema text id from whoosh import qparser from whoosh qparser import queryparser from whoosh analysis import stemminganalyzer import os define schema for indexing schema schema text stored true content text stored true analyzer stemminganalyzer stemming for better search results path id stored true unique true create index directory if it doesn t exist if not os path exists indexdir os mkdir indexdir create the index index create_in indexdir schema add documents to the index writer index writer example documents to index documents document 1 content the quick brown fox jumps over the lazy dog path a document 2 content whoosh is a fast search library implemented in pure python path b document 3 content the fox is quick and jumps high path c add each document to the index for doc in documents writer add_document doc content doc content path doc path writer commit save changes to the index print indexing completed search index bm25 usr bin env python from whoosh index import open_dir from whoosh qparser import queryparser def search query_str open the index ix open_dir indexdir parse the query with ix searcher as searcher query_parser queryparser content ix schema query query_parser parse query_str perform the search results searcher search query limit 10 print the results print f search results for query_str for result in results print f result path result path print f content result content print 40 example search queries search quick fox search search library output usr bin env python search results for quick fox document 3 path c content the fox is quick and jumps high document 1 path a content the quick brown fox jumps over the lazy dog search results for search library document 2 path b content whoosh is a fast search library implemented in pure python footer",
    "created_at": "2024-11-12T22:25:49.012552",
    "updated_at": "2024-11-12T22:25:49.012555"
  },
  {
    "url": "https://luzbetak.github.io/ai/PyTorch-Sentiment-Analysis-Model.html",
    "title": "PyTorch-Sentiment-Analysis-Model.html",
    "content": "sentiment analysis model build and save sentiment model usr bin env python import torch from torch utils data import dataset dataloader from torch import nn from transformers import berttokenizer bertmodel from sklearn metrics import accuracy_score classification_report define the textdataset class class textdataset dataset def __init__ self texts labels self texts texts self labels labels self tokenizer berttokenizer from_pretrained bert base uncased def __len__ self return len self texts def __getitem__ self idx text self texts idx label self labels idx tokens self tokenizer text padding max_length max_length 128 truncation true return_tensors pt input_ids tokens input_ids squeeze 0 attention_mask tokens attention_mask squeeze 0 return input_ids input_ids attention_mask attention_mask labels torch tensor label dtype torch float expanded dataset with both positive and negative examples text_data this movie is amazing positive i really disliked the plot negative the acting was superb positive this book is a masterpiece positive what a terrible waste of time negative i love this positive this was the worst movie ever negative fantastic experience loved it positive not good at all negative absolutely fantastic positive i hated every minute of it negative",
    "created_at": "2024-11-12T22:25:49.856273",
    "updated_at": "2024-11-12T22:25:49.856276"
  },
  {
    "url": "https://luzbetak.github.io/ai/Scikit-learn.html",
    "title": "Scikit-learn.html",
    "content": "scikit learn scikit learn overview scikit learn is a widely used open source python library for machine learning providing simple and efficient tools for data analysis and modeling it is built on top of popular libraries like numpy scipy and matplotlib and offers a wide range of algorithms for supervised and unsupervised learning key features preprocessing tools for scaling normalization encoding categorical variables and handling missing data model selection cross validation grid search and hyperparameter tuning for selecting the best models supervised learning algorithms like linear regression support vector machines svm decision trees and random forests unsupervised learning algorithms for clustering k means dbscan and dimensionality reduction pca t sne metrics a variety of metrics for evaluating model performance such as accuracy precision recall and auc roc model persistence allows saving trained models for later use via joblib or pickle scikit learn is highly accessible for both beginners and professionals with well documented apis and good integration with other data science tools like pandas from sklearn model_selection import train_test_split from sklearn datasets import load_iris from sklearn ensemble import randomforestclassifier from sklearn metrics import accuracy_score load the iris dataset data load_iris x data data features y data target target labels split the dataset into training and testing sets 80 train 20 test x_train x_test y_train y_test train_test_split x y test_size 0 2 random_state 42 initialize a random forest classifier clf randomforestclassifier n_estimators 100 random_state 42 train the classifier clf fit x_train y_train make predictions on the test set y_pred clf predict x_test calculate the accuracy of the model accuracy accuracy_score y_test y_pred print the accuracy print f accuracy accuracy 2f print a comparison of actual vs predicted values print nactual vs predicted for actual predicted in zip y_test y_pred print f actual actual predicted predicted print some sample data points from the test set to explain the results print nsample data points features from the test set for i in range 5 print f test sample i 1 x_test i predicted label y_pred i actual label y_test i output accuracy 1 00 actual vs predicted actual 1 predicted 1 actual 0 predicted 0 actual 2 predicted 2 actual 1 predicted 1 actual 1 predicted 1 actual 0 predicted 0 actual 1 predicted 1 actual 2 predicted 2 actual 1 predicted 1 actual 1 predicted 1 actual 2 predicted 2 actual 0 predicted 0 actual 0 predicted 0 actual 0 predicted 0 actual 0 predicted 0 actual 1 predicted 1 actual 2 predicted 2 actual 1 predicted 1 actual 1 predicted 1 actual 2 predicted 2 actual 0 predicted 0 actual 2 predicted 2 actual 0 predicted 0 actual 2 predicted 2 actual 2 predicted 2 actual 2 predicted 2 actual 2 predicted 2 actual 2 predicted 2 actual 0 predicted 0 actual 0 predicted 0 sample data points features from the test set test sample 1 6 1 2 8 4 7 1 2 predicted label 1 actual label 1 test sample 2 5 7 3 8 1 7 0 3 predicted label 0 actual label 0",
    "created_at": "2024-11-12T22:25:49.670021",
    "updated_at": "2024-11-12T22:25:49.670022"
  },
  {
    "url": "https://luzbetak.github.io/ai/Hugging-Face-Machine-Learning.html",
    "title": "Hugging-Face-Machine-Learning.html",
    "content": "hugging face machine learning hugging face login 1 login to hugging face cli if you haven t logged in yet authenticate via the command line huggingface cli login this will prompt you to enter your hugging face token you can generate a token from your hugging face account go to https huggingface co settings tokens create a new token with read access to the repository 2 verify repository access ensure you have the necessary permissions to access the restricted model visit the model page https huggingface co black forest labs flux 1 dev if the model is gated request access and wait for approval from the maintainers 3 set up api token if you re accessing the model programmatically configure the token properly export huggingface_token your_token_here or set it directly in your script from huggingface_hub import hfapi api hfapi api login token your_token_here 4 retry model download once authenticated run the command or script again to access the model note if you still encounter issues double check that the model owners have granted access if not you ll need to wait for their approval footer",
    "created_at": "2024-11-12T22:25:48.902390",
    "updated_at": "2024-11-12T22:25:48.902392"
  },
  {
    "url": "https://luzbetak.github.io/ai/bm25-probabilistic-information-retrieval-model.html",
    "title": "bm25-probabilistic-information-retrieval-model.html",
    "content": "bm25 probabilistic information retrieval model bm25 best matching 25 is a ranking function used by search engines to rank documents based on their relevance to a given query it is one of the most well known algorithms within the family of probabilistic information retrieval models bm25 builds upon the earlier tf idf term frequency inverse document frequency approach and is considered a highly effective ranking model for information retrieval tasks key components of bm25 term frequency tf measures how often a term keyword appears in a document bm25 uses a variant called saturated term frequency meaning that the importance of term frequency grows less rapidly as it increases this prevents overemphasis on documents with high repetition of a term inverse document frequency idf measures how unique or rare a term is across the entire document collection corpus terms that appear in many documents are given less weight while terms that appear in fewer documents are considered more important document length normalization bm25 takes into account the length of a document to avoid favoring longer documents that may mention terms more frequently simply because they contain more content the algorithm normalizes the term frequency based on document length adjusting the importance of terms in shorter or longer documents bm25 formula the bm25 relevance score for a document d and a query q is calculated as bm25 d q t q idf t f t d k1 1 f t d",
    "created_at": "2024-11-12T22:25:49.309389",
    "updated_at": "2024-11-12T22:25:49.309392"
  },
  {
    "url": "https://luzbetak.github.io/ai/Vector-Database.html",
    "title": "Vector-Database.html",
    "content": "vector database vector database a vector database is a specialized type of database designed to efficiently store retrieve and query data in vector format vectors often representing numerical or feature embeddings from high dimensional data e g images text audio are used extensively in machine learning models these embeddings capture the essential characteristics of the data such as its semantic meaning by encoding it in vector space usage of vector databases in machine learning vector databases play a critical role in machine learning tasks where similarity search or clustering of high dimensional data is needed common usage scenarios recommendation systems retrieve similar items by finding nearest neighbors in vector space natural language processing nlp retrieve similar documents based on text embeddings image retrieval perform similarity search based on image embeddings anomaly detection identify abnormal behavior by clustering event vectors common machine learning techniques using vector databases nearest neighbor search k nn retrieve the nearest vectors for classification and regression tasks clustering k means dbscan store vectors for efficient clustering semantic search search for semantically similar text using text embeddings text embedding search store embeddings from models like bert or gpt to find similar documents image search store image embeddings for visual search applications recommendation systems recommend content based on similar user or item embeddings anomaly detection identify outliers in behavior or transaction data by detecting anomalies in vector space lancedb vector database overview lancedb is an open source vector database designed for efficient and fast storage retrieval and management of high dimensional vectors it focuses on providing real time performance and scalability for machine learning and ai applications lancedb is built for handling vector search workloads allowing users to store embeddings from text images or other data types and perform similarity searches with high efficiency key features open source and local hosting no api key required meaning it can be run locally or self hosted for full control optimized for vector search built specifically for storing vectors and performing nearest neighbor searches scalability lancedb can handle a wide range of workloads from small scale applications to large scale production environments integration with machine learning pipelines lancedb integrates well with ml pipelines making it ideal for ai driven applications such as recommendation systems semantic search and more real time search performance focuses on low latency queries and high throughput for fast real time vector searches use cases recommendation systems store embeddings of users or items and perform similarity searches to recommend products content or services semantic search use embeddings from nlp models like bert to find similar documents based on meaning rather than just keywords image search store image embeddings and retrieve similar images using vector similarity anomaly detection identify unusual data points by storing event vectors and detecting outliers using clustering and similarity searches why use lancedb performance oriented built to handle the performance needs of real time vector search applications machine learning friendly specifically designed to fit within the machine learning ecosystem making it easy to integrate with modern ai pipelines self hosted gives users full control over their data without the need for external apis or services python code using lancedb pip install lancedb import lancedb import numpy as np initialize the lancedb database db lancedb connect path to lancedb specify the path where the database will be stored create or connect to a collection similar to a table in a traditional db collection db create_collection example_collection generate random vector data e g 1000 vectors of dimensionality 128 vectors np random rand 1000 128 tolist insert vectors with associated metadata ids or labels data id i vector vec for i vec in enumerate vectors collection insert data query the collection for the nearest neighbors of a new vector query_vector np random rand 1 128 tolist 0 generate a random query vector results collection search query_vector limit 5 to_list limit the result to top 5 display the nearest neighbors and their distances for result in results print f id result id distance result distance key steps in the code connect to lancedb initializes a connection to lancedb at a specified path it can be local or a remote directory create a collection creates or connects to a collection which acts like a table for storing vectors and metadata insert data inserts 1000 randomly generated vectors into the collection each associated with an id query for nearest neighbors uses a randomly generated query vector to search for the top 5 most similar vectors in the collection explanation vectors in this example 1000 random vectors of dimensionality 128 are generated in real world applications these vectors could represent embeddings from text images or other high dimensional data search the search function performs a nearest neighbor search to find the most similar vectors in the collection based on distance e g euclidean or cosine similarity faiss facebook ai similarity search faiss is an open source library developed by facebook ai for efficient similarity search and clustering of dense vectors it is highly optimized for handling very large datasets of high dimensional vectors key features no api key required since it s a local library scales to billions of vectors offers various index types such as flat hnsw and ivf inverted file index supports both gpu and cpu for faster performance use case faiss can be used in scenarios like image search recommendation systems and nlp embedding retrieval pip install faiss cpu or faiss gpu for gpu version import faiss import numpy as np create an index for l2 distance euclidean d 128 dimension of vectors index faiss indexflatl2 d create the index generate random data vectors vectors np random random 1000 d astype float32 add vectors to the index index add vectors query for the nearest neighbors query_vector np random random 1 d astype float32 distances indices index search query_vector k 5 retrieve top 5 nearest neighbors print nearest neighbors indices annoy approximate nearest neighbors overview annoy is an open source library developed by spotify for finding approximate nearest neighbors it s designed for situations where you want fast search with large datasets but are willing to trade some accuracy for performance key features no api key required optimized for in memory storage and fast querying supports various distance metrics like euclidean manhattan and angular distance pip install annoy from annoy import annoyindex import numpy as np create an index with 128 dimensional vectors and angular distance metric f 128 dimension of vectors index annoyindex f angular add vectors to the index for i in range 1000 vector np random random f tolist index add_item i vector build the index tree count affects speed accuracy index build 10 query for the nearest neighbors nearest_neighbors index get_nns_by_item 0",
    "created_at": "2024-11-12T22:25:49.947482",
    "updated_at": "2024-11-12T22:25:49.947485"
  },
  {
    "url": "https://luzbetak.github.io/ai/Time-Complexity-Big-O-Notation.html",
    "title": "Time-Complexity-Big-O-Notation.html",
    "content": "kevin luzbetak computer science big o notation time complexity big o notation is used to describe the efficiency of an algorithm focusing on its time complexity how the execution time grows with input size and space complexity how much extra memory is needed it expresses the worst case scenario performance of an algorithm common complexities o 1 constant time the algorithm s runtime does not change with the input size example accessing an element in an array by index o log n logarithmic time the algorithm reduces the problem size by a constant factor with each step example binary search in a sorted array o n linear time the runtime grows proportionally with the input size example traversing a list of n elements o n log n linearithmic time the algorithm performs a linear number of operations for each logarithmic division example merge sort and quicksort in their average cases o n² quadratic time the runtime grows quadratically with input size example a double nested loop like in bubble sort or selection sort o 2 n exponential time the runtime doubles with each addition to the input size example solving the traveling salesman problem using brute force o n factorial time the runtime increases factorially with the input size example generating all permutations of a set data structures and their time complexities hash table access o 1 average case o n worst case due to collisions search o 1 average case insertion deletion o 1 average case b tree access search o log n insertion deletion o log n space complexity o n balanced binary search tree e g avl tree red black tree access search o log n insertion deletion o log n binary search tree bst access search o log n average o n worst case unbalanced insertion deletion o log n average o n worst case b tree balancing b trees are inherently balanced during indexing so they do not require rebalancing in the way that some other tree structures like avl trees or red black trees do balanced nature b trees are self balancing by design when you insert or delete keys the tree is adjusted to maintain its balance this is achieved by splitting or merging nodes as necessary during insertions and deletions ensuring that all leaf nodes remain at the same level and the tree remains balanced indexing during the indexing process as new keys are inserted into the b tree the structure automatically ensures that the tree remains balanced this is done by maintaining certain properties such as all nodes except the root having at least a minimum number of children the height of the tree being kept as low as possible to optimize search operations because of this b trees are particularly well suited for use in databases and file systems where efficient data retrieval and storage are critical array fixed size access o 1 search o n insertion o n if resizing required deletion o n linked list access o n search o n insertion deletion at head o 1 heap priority queue access max or min o 1 insertion deletion o log n graph adjacency matrix adjacency list search dfs bfs o v e where v is the number of vertices and e is the number of edges stack queue access o n insertion deletion o 1 footer",
    "created_at": "2024-11-12T22:25:49.360118",
    "updated_at": "2024-11-12T22:25:49.360121"
  },
  {
    "url": "https://luzbetak.github.io/ai/TF-IDF.html",
    "title": "TF-IDF.html",
    "content": "tf idf usr bin env python tf idf term frequency inverse document frequency from sklearn feature_extraction text import tfidfvectorizer from prettytable import prettytable example documents docs this is a sample document this document is another sample document machine learning document initialize the vectorizer vectorizer tfidfvectorizer fit the model and transform the text data into a tf idf matrix tfidf_matrix vectorizer fit_transform docs get feature names terms feature_names vectorizer get_feature_names_out convert to array to see the result tfidf_array tfidf_matrix toarray initialize prettytable table prettytable add column names terms table field_names document list feature_names add rows documents and their tf idf values rounded to 4 decimals for i doc in enumerate docs row f document i 1 round value 4 for value in tfidf_array i table add_row row print table document another document is learning machine sample this document 1 0 0 0 4091 0 5268 0 0 0 0 0",
    "created_at": "2024-11-12T22:25:49.527524",
    "updated_at": "2024-11-12T22:25:49.527526"
  },
  {
    "url": "https://luzbetak.github.io/ai/Gunning-Fog-Index.html",
    "title": "Gunning-Fog-Index.html",
    "content": "python algorithms gunning fog index the gunning fog index is a readability test that estimates the years of formal education needed to understand a text on the first reading it takes into account the number of words the number of complex words words with three or more syllables and the number of sentences in a text import re def count_words text counts the number of words in a text words re findall r w text return len words def count_sentences text counts the number of sentences in a text sentences re split r text return len sentences 1 if text 1 in else len sentences def count_complex_words text counts the number of complex words in a text words with three or more syllables def syllable_count word word word lower vowels aeiouy count 0 if word 0 in vowels count 1 for index in range 1 len word if word index in vowels and word index 1 not in vowels count 1 if word endswith e count 1 if count 0 count 1 return count words re findall r w text complex_words word for word in words if syllable_count word 3 return len complex_words def gunning_fog_index text calculates the gunning fog index for a given text num_words count_words text num_sentences count_sentences text num_complex_words count_complex_words text if num_sentences 0 return 0 avoid division by zero average_sentence_length num_words num_sentences percentage_complex_words",
    "created_at": "2024-11-12T22:25:49.259196",
    "updated_at": "2024-11-12T22:25:49.259198"
  },
  {
    "url": "https://luzbetak.github.io/ai/NVIDIA-GeForce-RTX-3060-Installation.html",
    "title": "NVIDIA-GeForce-RTX-3060-Installation.html",
    "content": "geforce rtx 3060 installation install nvidia geforce rtx 3060 for machine learning step 1 prepare your system before installing the nvidia drivers update your system and install the necessary packages sudo apt update sudo apt upgrade install required build tools and headers sudo apt install build essential linux headers uname r step 2 install nvidia drivers add the nvidia ppa add the official nvidia ppa to get the latest drivers sudo add apt repository ppa graphics drivers ppa sudo apt update identify the recommended driver find the recommended driver for your gpu using the following command ubuntu drivers devices the output will suggest the best driver for your system for example it may recommend nvidia driver 560 install the recommended driver replace nvidia driver xxx with the recommended driver such as nvidia driver 560 sudo apt install nvidia driver 560 reboot your system reboot your machine to apply the changes sudo reboot step 3 install cuda optional if you plan to use cuda for gpu computing you can install it using the following command sudo apt install nvidia cuda toolkit step 4 verify installation after rebooting you can check if the driver is installed and working properly by running alias nv1 nvtop alias nv2 nvidia smi this will display details about your installed gpu and the driver version step 5 enable microphone on chrome insecure origins treated as secure http 192 168 1 190 8080 chrome flags footer",
    "created_at": "2024-11-12T22:25:48.945286",
    "updated_at": "2024-11-12T22:25:48.945289"
  },
  {
    "url": "https://luzbetak.github.io/ai/Random-Forest-Classifier-Model.html",
    "title": "Random-Forest-Classifier-Model.html",
    "content": "random forest classifier model random forest classifieri with tf idf vectorizer this model consists of a collection of decision trees the forest where each tree is trained on a random subset of the data the final prediction is made by averaging the predictions of all the individual trees which helps reduce overfitting and improves generalization input features the text data customer questions is vectorized using the tf idf vectorizer tfidfvectorizer to convert the textual information into numerical features that the model can understand task the model is trained to classify customer support questions into various categories such as product support billing order etc given a new customer query the model can predict which category it belongs to usr bin env python import pandas as pd from sklearn model_selection import train_test_split from sklearn feature_extraction text import tfidfvectorizer from sklearn ensemble import randomforestclassifier from sklearn metrics import accuracy_score from prettytable import prettytable step 1 create data model dictionary how do i reset my password product support what are the payment options billing where can i find my purchase history billing how can i contact support product support can i get a refund billing why was i charged twice billing how do i change my subscription product support what is the return policy billing how do i track my order product support how do i update my billing information billing how do i troubleshoot my device product support why is my payment not going through billing how can i cancel my order product support what should i do if i receive a damaged item product support where can i see my account balance billing how do i apply a discount code billing can i exchange a product i purchased product support how do i upgrade my plan product support why was my account suspended product support can i change the shipping address for my order product support where do i find the user manual product support how do i report a missing item in my order product support how do i enable two factor authentication product support what is the warranty on my product product support how do i pay my outstanding balance billing how can i verify my payment method billing why was my refund declined billing can i pay using cryptocurrency billing how do i dispute a charge billing where do i see my payment history billing what do i do if i forgot my login credentials product support how do i reset my account pin product support what happens if my subscription expires product support where can i view my invoice billing how do i access technical support product support what are the supported payment methods billing can i change my delivery date product support how do i redeem a gift card billing how do i unsubscribe from email notifications billing what do i do if my card is declined billing can i update my order before it ships product support how can i find the nearest store billing why is my bill higher than expected billing what should i do if i forgot my password product support how do i cancel my subscription product support how do i track my refund billing why was my payment declined billing how do i contact technical support product support how do i change my account details product support ensure the lists have the same length df pd dataframe list dictionary items columns question category step 2 train test split x_train x_test y_train y_test train_test_split df question df category test_size 0 3 random_state 42 step 3 vectorize text data vectorizer tfidfvectorizer x_train_vec vectorizer fit_transform x_train",
    "created_at": "2024-11-12T22:25:49.914073",
    "updated_at": "2024-11-12T22:25:49.914076"
  },
  {
    "url": "https://luzbetak.github.io/ai/Streamlit-app.html",
    "title": "Streamlit-app.html",
    "content": "streamlit app streamlit app enter your name select your age 25 hello kevin you are 25 years old line chart a b c 0 17 0 57 1 22 0 47 1 11 0 34 0 33 0 88 0 45 1 12 1 34 0 76 1 02 0 66 0 99 usr bin env python import streamlit as st import pandas as pd import numpy as np of the app st streamlit app text input box user_name st text_input enter your name kevin slider input age st slider select your age 1 100 25 display user input st write f",
    "created_at": "2024-11-12T22:25:49.083882",
    "updated_at": "2024-11-12T22:25:49.083884"
  },
  {
    "url": "https://luzbetak.github.io/ai/Stable-Diffusion-Web-UI.html",
    "title": "Stable-Diffusion-Web-UI.html",
    "content": "stable diffusion web ui stable diffusion web ui 1 clone the web ui repository stable diffusion webui git clone https github com automatic1111 stable diffusion webui git cd stable diffusion webui 2 install dependencies pip install r requirements txt if some dependencies fail try installing them manually pip install torch torchvision torchaudio index url https download pytorch org whl cpu 3 launch the web ui run the following command to start the web ui python launch py if you encounter the torch is not able to use gpu error use this command export commandline_args skip torch cuda test python launch py 4 access the web ui in your browser once the server is running the terminal will display a message like running on local url http 127 0 0 1 7860 open your web browser and go to http 127 0 0 1 7860 5 stopping the server to stop the web ui press ctrl c in the terminal where it s running optional run with mps for mac users if you re using a mac with an m1 m2 chip run with mps acceleration export pytorch_enable_mps_fallback 1 python launch py footer",
    "created_at": "2024-11-12T22:25:49.185321",
    "updated_at": "2024-11-12T22:25:49.185324"
  },
  {
    "url": "https://luzbetak.github.io/ai/Recompile-FAISS-GPU-Installation.html",
    "title": "Recompile-FAISS-GPU-Installation.html",
    "content": "rebuild cuda gpu faiss gpu installation step 1 install dependencies run the following commands to install the necessary dependencies sudo apt get update sudo apt get install y cmake libopenblas dev libomp dev libgtest dev gcc 10 g 10 step 2 install cuda if cuda is not installed follow these steps to install it sudo apt get install cuda 12 2 set up environment variables for cuda export cuda_home usr local cuda 12 2 export path cuda_home bin path export ld_library_path cuda_home lib64 ld_library_path step 3 clone faiss repository git clone https github com facebookresearch faiss git cd faiss step 4 file editing modify cmakelists txt edit the cmakelists txt file to ensure proper configuration vi faiss cmakelists txt add the following lines at the top cmake_minimum_required version 3 27 project faiss version 1 7 0 languages c cxx fix the path for faiss config cmake in by editing the line around line 410 in cmakelists txt configure_file cmake_source_dir cmake faiss config cmake in cmake_binary_dir faiss config cmake only step 5 build faiss with gpu support cmake b build dfaiss_enable_gpu on dcmake_cuda_compiler usr local cuda 12 2 bin nvcc dcmake_cuda_architectures 86 dbuild_testing off dcmake_build_type release make j nproc step 6 install faiss optional sudo make install step 7 verify faiss installation run the following test script to verify faiss installation import faiss import numpy as np create random vectors d 128 dimension nb 1000 number of vectors xb np random random nb d astype float32 create an index for l2 distance index faiss indexflatl2 d add vectors index add xb search for nearest neighbors d i index search xb 1 5 print",
    "created_at": "2024-11-12T22:25:49.447684",
    "updated_at": "2024-11-12T22:25:49.447687"
  },
  {
    "url": "https://luzbetak.github.io/ai/PyTorch.html",
    "title": "PyTorch.html",
    "content": "pytorch neural network pytorch machine learning library pytorch is an open source machine learning library used for a wide variety of tasks such as deep learning natural language processing nlp and computer vision it provides a flexible platform to build machine learning models and comes with strong support for gpu acceleration making it popular among researchers and developers key features of pytorch tensors pytorch provides multi dimensional arrays similar to numpy but with gpu support for faster computation autograd pytorch uses automatic differentiation to compute gradients making it easier to implement and optimize neural networks dynamic computational graphs pytorch builds computational graphs dynamically making it easier to debug and modify deep learning models pytorch includes pre built models in the torchvision and torchtext libraries for various computer vision and nlp tasks extensibility you can build custom layers models and optimization techniques pytorch example how to use pytorch to create a neural network for classifying mnist digits usr bin env python import torch import torch nn as nn import torch optim as optim import torchvision import torchvision transforms as transforms define a simple neural network class net nn module def __init__ self super net self __init__ self fc1 nn linear 28 28 128 fully connected layer 1 self fc2 nn linear 128 10 fully connected layer 2 10 classes def forward self x x x view 1 28 28 flatten the image x torch relu self fc1 x apply relu to fc1 x self fc2 x final output logits return x load dataset and preprocess transform transforms compose transforms totensor transforms normalize 0 5 0 5 trainset torchvision datasets mnist root data train true download true transform transform trainloader torch utils data dataloader trainset batch_size 64 shuffle true initialize the model loss function and optimizer net net criterion nn crossentropyloss optimizer optim sgd net parameters lr 0 01 training loop for epoch in range 2 running_loss 0 0 for inputs labels in trainloader optimizer zero_grad zero the gradients outputs net inputs forward pass loss criterion outputs labels compute the loss loss backward backward pass optimizer step optimize running_loss loss item print f epoch epoch 1 loss running_loss len trainloader print finished training common pytorch libraries torchvision contains datasets models and transforms for computer vision torchtext for nlp models and datasets torchaudio for audio and speech processing footer",
    "created_at": "2024-11-12T22:25:49.656741",
    "updated_at": "2024-11-12T22:25:49.656744"
  },
  {
    "url": "https://luzbetak.github.io/ai/Retrieval-Augmented-Generation.html",
    "title": "Retrieval-Augmented-Generation.html",
    "content": "rag retrieval augmented generation building a high performance rag solution with pgvectorscale and python 1 rag retrieval augmented generation rag enhances the response generation process by retrieving relevant documents from an external knowledge base e g a vector database and using these documents to inform the generated responses it combines retriever finds relevant documents based on a query generator uses the retrieved documents to generate a more accurate and informed response 2 pgvectorscale pgvectorscale is an extension for postgresql that enables high performance vector similarity searches using embeddings it builds on pgvector allowing storage and indexing of high dimensional vectors making it suitable for large scale rag systems 3 setting up the components to build the rag solution you ll need a vector database postgresql with pgvector pgvectorscale to store embeddings embeddings for your documents generated via models like gpt bert etc a retrieval algorithm to find relevant documents a generation model such as gpt to use the retrieved documents for response generation 4 build a high performance rag solution step 1 install postgresql pgvector and pgvectorscale install the necessary components create extension vector create extension pgvectorscale step 2 generate embeddings use a model to create embeddings for your documents from sentence_transformers import sentencetransformer model sentencetransformer all minilm l6 v2 documents your document text here another document text embeddings model encode documents step 3 store embeddings in postgresql create a table to store the embeddings create table documents id serial primary key text text embedding vector 768 insert documents and embeddings import psycopg2 conn psycopg2 connect dbname test user postgres cur conn cursor cur execute insert into documents text embedding values s s document_text embedding tolist conn commit step 4 retrieve relevant documents query the vector database for the most relevant documents select from documents order by embedding query_embedding limit 5 step 5 generate responses using the retrieved documents use a generation model like gpt to create a response from transformers import gpt2lmheadmodel gpt2tokenizer model gpt2lmheadmodel from_pretrained gpt2 tokenizer gpt2tokenizer from_pretrained gpt2 prompt f based on the following documents n retrieved_documents nanswer the question user_query inputs tokenizer prompt return_tensors pt outputs model generate inputs input_ids max_length 500 response tokenizer decode outputs 0 print response 5 optimization for high performance to ensure high performance use indexing and parallel queries in postgresql for large scale datasets distribute the retrieval tasks across multiple nodes 6 use cases customer support augment chatbot answers with external documents search engines provide contextual responses to user queries knowledge management retrieve and synthesize information from large repositories references high performance rag solution with pgvectorscale and python tutorial build high performance rag footer",
    "created_at": "2024-11-12T22:25:49.755465",
    "updated_at": "2024-11-12T22:25:49.755467"
  },
  {
    "url": "https://luzbetak.github.io/ai/Tensors-Machine-Learning.html",
    "title": "Tensors-Machine-Learning.html",
    "content": "tensors multi dimensional array tensor in machine in essence a tensor is a multidimensional array used to represent data think of it as a generalization of vectors and matrices a scalar a single number is a 0 dimensional tensor a vector a list of numbers is a 1 dimensional tensor a matrix a table of numbers is a 2 dimensional tensor and we can have tensors with 3 4 or even more dimensions tensors are the fundamental data structure in deep learning frameworks like tensorflow and pytorch they allow us to efficiently represent and process complex data like images 3d tensors height width color channels videos 4d tensors time height width color channels and even natural language where words are embedded in high dimensional spaces simple python code with tensors using numpy numpy is a powerful library for numerical computations in python and provides excellent support for working with tensors which numpy calls ndarrays import numpy as np create a 2d tensor a matrix matrix np array 1 2 3 4 5 6 create a 3d tensor tensor_3d np array 1 2 3 4 5 6 7 8 access elements of the tensors print element at row 1 column 2 of the matrix matrix 1 2 print element at depth 0 row 1 column 0 of the 3d tensor tensor_3d 0 1 0 perform operations on tensors sum_of_matrix np sum matrix print sum of all elements in the matrix sum_of_matrix output element at row 1 column 2 of the matrix 6 element at depth 0 row 1 column 0 of the 3d tensor 3 sum of all elements in the matrix 21 in this code import the numpy library create a 2d tensor matrix and a 3d tensor tensor_3d using np array access specific elements using indexing remember indexing starts at 0 perform an operation summation on the matrix using np sum key points tensors enable us to represent and manipulate data with any number of dimensions deep learning frameworks heavily rely on tensors for efficient computation on gpus numpy provides a solid foundation for working with tensors in python import numpy as np create a 3d tensor with dimensions 2 3 4 tensor np zeros 2 3 4 fill the tensor with some values tensor 0 0 0 1 tensor 0 1 1 2 tensor 0 2 2 3 tensor 1 0 3 4 tensor 1 1 2 5 tensor 1 2 1 6 print the tensor print tensor access a specific value in the tensor e g the value at index 1 0 3 value tensor 1 0 3 print value sum all the values in the tensor total np sum tensor print total compute the mean of all the values in the tensor average np mean tensor print average in this example we create a 3d tensor with dimensions 2 3 4 using the np zeros function we then fill the tensor with some values using indexing we print the tensor access a specific value compute the sum and mean of all the values in the tensor using the np sum and np mean functions note that 3d tensors are also commonly used in deep learning frameworks such as tensorflow or pytorch the syntax for creating and manipulating 3d tensors in those frameworks is similar to numpy but with additional functionality for building and training machine learning models footer",
    "created_at": "2024-11-12T22:25:49.933202",
    "updated_at": "2024-11-12T22:25:49.933204"
  },
  {
    "url": "https://luzbetak.github.io/ai/faiss/Indexing-FAISS-OpenAI-Embeddings.html",
    "title": "Indexing-FAISS-OpenAI-Embeddings.html",
    "content": "email processing with faiss and openai embeddings email processing with faiss and openai embeddings description this python script performs the following tasks loads email data from a json file full_emails json extracts relevant fields such as content id and timestamp using a schema cleans the email content by removing urls extra spaces and unnecessary text converts the cleaned content into document objects for further processing generates embeddings for the email documents using openaiembeddings stores the embeddings in a faiss vector database for fast similarity search saves the faiss index to disk for future use usr bin env python import json import re import os from datetime import datetime from langchain_community document_loaders import jsonloader from langchain_openai import openaiembeddings updated import from langchain_community vectorstores import faiss from langchain schema import document import document class define jq schema to extract relevant fields from the json jq_schema page_content content metadata id id date date initialize the jsonloader with the schema loader jsonloader full_emails json jq_schema jq_schema text_content false documents loader load initialize openai embeddings embeddings openaiembeddings clean email content function def clean_content text text re sub r http s text remove urls text re sub r 2 text replace sequences of dashes with one dash text re sub r unsubscribe learn why we included you are receiving text flags re ignorecase text re sub r image w text remove entities return re sub r s text strip remove extra spaces convert the cleaned content into document objects processed_docs for doc in documents try content_data json loads doc page_content parse content content clean_content content_data get page_content no content metadata content_data get metadata email_id metadata get id unknown id timestamp int metadata get date 0 1000 convert to seconds date datetime fromtimestamp timestamp isoformat if timestamp 0 else unknown create document object processed_docs append document page_content content metadata id email_id date date except json jsondecodeerror as e print f error parsing json content e create faiss vector store with the document objects vector_store faiss from_documents processed_docs embeddings save the faiss index to disk faiss_index_path faiss_index vector_store save_local faiss_index_path print f faiss index saved to faiss_index_path explanation of the code this script processes a set of emails and builds a vector index for efficient search and retrieval using faiss jsonloader loads email data from the json file using the provided schema to extract content id and timestamp cleaning content the clean_content function removes unwanted elements like urls extra dashes and boilerplate text e g unsubscribe messages document objects each email is converted into a document object which stores both the cleaned content and metadata openai embeddings generates vector embeddings for each document to enable similarity search faiss vector store stores the documents embeddings in a faiss index for fast searching saving the index the faiss index is saved locally for future searches output when the script is run it displays the following message faiss index saved to faiss_index conclusion this script leverages openai embeddings and the faiss vector store to process and index email data allowing for efficient similarity searches this setup can be extended to support various document types or integrate with other machine learning models for advanced use cases footer",
    "created_at": "2024-11-12T22:25:49.927348",
    "updated_at": "2024-11-12T22:25:49.927350"
  },
  {
    "url": "https://luzbetak.github.io/ai/faiss/RetrievalQA-FAISS-with-OpenAI-GPT-4.html",
    "title": "RetrievalQA-FAISS-with-OpenAI-GPT-4.html",
    "content": "faiss retrieval qa with openai gpt 4 retrievalqa retrievalqa is a specialized question answering framework that leverages retrievers like faiss or vector databases to find relevant documents or text snippets based on a query and then uses language models llms to generate answers based on the retrieved content this architecture improves the llm s ability to answer questions accurately by limiting the scope to relevant data instead of relying on the model s general knowledge in this framework retriever quickly fetches relevant documents or information from a knowledge base e g a faiss index language model llm uses retrieved content as context to generate precise answers to user questions faiss retrieval qa with openai gpt 4 description this python script builds a conversational qa system by integrating faiss vector stores with openai gpt 4 it retrieves relevant documents from the faiss index to answer user questions interactively below is a step by step breakdown of the script s functionality script breakdown api key setup the openai api key is retrieved from an environment variable openai_api_key if the key is not set the script raises an error loading faiss index the faiss vector index is loaded from disk allowing similarity search across indexed content if the index is missing the script raises a filenotfounderror initializing gpt 4 uses the chatopenai model to provide answers based on the retrieved documents building the qa chain a retrievalqa chain is constructed to connect the faiss retriever and the gpt 4 model for end to end question answering query loop the user can interact with the system in a loop asking questions if relevant documents are found the answer is displayed if no documents match the user is notified python code usr bin env python import os import openai ensure openai is imported from langchain_openai import openaiembeddings chatopenai updated imports from langchain_community vectorstores import faiss from langchain chains import retrievalqa step 1 set up api key openai api_key os getenv openai_api_key if not openai api_key raise valueerror openai api key not found set openai_api_key as an environment variable step 2 load the faiss index from disk faiss_index_path faiss_index embeddings openaiembeddings if os path exists faiss_index_path print f loading faiss index from faiss_index_path",
    "created_at": "2024-11-12T22:25:49.954362",
    "updated_at": "2024-11-12T22:25:49.954363"
  },
  {
    "url": "https://luzbetak.github.io/ai/faiss/Gmail-Email-Fetch.html",
    "title": "Gmail-Email-Fetch.html",
    "content": "gmail email fetch gmail email fetch script description this python script uses the gmail api to authenticate a user and retrieve the last 1000 emails from their gmail account the email data is saved as a json file on the local system below is a breakdown of the process and the complete script script breakdown oauth2 authentication uses google oauth credentials to authenticate the user if valid credentials are found in token json they are reused otherwise a new login prompt is initiated fetching emails after authentication the script retrieves up to 1000 emails and extracts key details such as email id snippet of content internal date timestamp payload metadata saving emails all fetched email data is stored in last_1000_emails json with readable formatting usr bin env python import os import json from google oauth2 credentials import credentials from google_auth_oauthlib flow import installedappflow from googleapiclient discovery import build define the oauth scope for read only gmail access scopes https www googleapis com auth gmail readonly authenticate and create gmail api service def authenticate_gmail creds none if os path exists token json creds credentials from_authorized_user_file token json scopes if there are no valid credentials authenticate the user if not creds or not creds valid flow installedappflow from_client_secrets_file credentials json scopes creds flow run_local_server port 0 save credentials for future runs with open token json w as token_file token_file write creds to_json return build gmail v1 credentials creds fetch the last 1000 emails and save them to a json file def fetch_and_dump_emails service authenticate_gmail results service users messages list userid me maxresults 1000 execute messages results get messages emails for msg in messages msg_detail service users messages get userid me id msg id execute email_data id msg_detail id snippet msg_detail get snippet internaldate msg_detail get internaldate payload msg_detail get payload emails append email_data save the emails to a json file with open last_1000_emails json w as f json dump emails f indent 2 print successfully dumped the last 1000 emails to last_1000_emails json run the script if __name__ __ main__",
    "created_at": "2024-11-12T22:25:49.931619",
    "updated_at": "2024-11-12T22:25:49.931621"
  }
]